{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models.rnn import RecurrentNetwork\n",
    "from pytorch_forecasting.data.encoders import EncoderNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "data = pd.read_csv('dataset/preliminaryData/bss_activity_meteorological_popular-hours.csv')\n",
    "\n",
    "max_prediction_length = 7*6\n",
    "max_encoder_length = 15*6\n",
    "num_workers=32\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "data = data[lambda x: x.time_idx < data[\"time_idx\"].max() - 30*4*6] # drop celebration days\n",
    "data[\"station\"] = data[\"station\"].astype(str)\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data[\"weekday\"] = data[\"weekday\"].astype(str)\n",
    "data[\"is_weekend\"] = data[\"is_weekend\"].astype(str)\n",
    "data[\"time_of_day\"] = data[\"time_of_day\"].astype(str)\n",
    "data[\"season\"] = data[\"season\"].astype(str)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx < training_cutoff],\n",
    "    group_ids=[\"station\"],\n",
    "    target=\"activity\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[\"activity\"],\n",
    "    static_categoricals=[\"station\"],\n",
    "    time_varying_known_categoricals=[\"weekday\", \"is_weekend\", \"time_of_day\", \"month\"],\n",
    "    time_varying_known_reals=[\"is_public_hours\"],\n",
    "    target_normalizer=EncoderNormalizer(transformation=\"softplus\"),\n",
    "    lags={\"activity\": [6, 6*7,6*365]},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=num_workers)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=num_workers)\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting.metrics.point import MAE\n",
    "import optuna.logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "model_path = \"study_lstm\"\n",
    "log_dir = \"study_lstm\"\n",
    "gradient_clip_val_range = [0.01, 1.0]\n",
    "dropout_range = (0.1, 0.5)\n",
    "rnn_layers = (1, 10)\n",
    "hidden_size = (8, 128)\n",
    "learning_rate_range = (1e-4, 1e-1)\n",
    "n_trials = 50\n",
    "max_epochs = 50\n",
    "study = None\n",
    "\n",
    "class PyTorchLightningPruningCallbackAdjusted(pl.Callback, PyTorchLightningPruningCallback):\n",
    "    pass\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Filenames for each trial must be made unique in order to access each checkpoint.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=os.path.join(model_path, \"trial_{}\".format(trial.number)), filename=\"{epoch}\", monitor=\"val_loss\"\n",
    "    )\n",
    "\n",
    "    learning_rate_callback = LearningRateMonitor()\n",
    "    logger = TensorBoardLogger(log_dir, name=\"optuna\", version=trial.number)\n",
    "    gradient_clip_val = trial.suggest_loguniform(\"gradient_clip_val\", *gradient_clip_val_range)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=max_epochs,\n",
    "        gradient_clip_val=gradient_clip_val,\n",
    "        callbacks=[\n",
    "            learning_rate_callback,\n",
    "            checkpoint_callback,\n",
    "            PyTorchLightningPruningCallbackAdjusted(trial, monitor=\"val_loss\"),\n",
    "        ],\n",
    "        logger=logger,\n",
    "        limit_train_batches=30, \n",
    "        devices=1,\n",
    "        enable_progress_bar=optuna.logging.INFO,\n",
    "    )\n",
    "\n",
    "    # create model\n",
    "    model = RecurrentNetwork.from_dataset(\n",
    "        train_dataloader.dataset,\n",
    "        dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
    "        cell_type='LSTM',\n",
    "        learning_rate=0.06,\n",
    "        loss=MAE(),\n",
    "        rnn_layers=trial.suggest_int(\"rnn_layers\", *rnn_layers),\n",
    "        hidden_size=trial.suggest_int(\"hidden_size\", *hidden_size, log=True),\n",
    "        optimizer=\"Ranger\",\n",
    "        log_interval=-1,\n",
    "    )\n",
    "    model.hparams.learning_rate = trial.suggest_loguniform(\"learning_rate\", *learning_rate_range)\n",
    "\n",
    "    # fit\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    # report result\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "# setup optuna and run\n",
    "if study is None:\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "with open(\"study_lstm_2.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "logger = TensorBoardLogger(\"study\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto', \n",
    "    enable_model_summary=True,\n",
    "    callbacks=[early_stop_callback],\n",
    "    gradient_clip_val=0.02,\n",
    "    logger=logger,\n",
    "    limit_train_batches=30,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "net = RecurrentNetwork.from_dataset(\n",
    "    training,\n",
    "    cell_type='LSTM',\n",
    "    learning_rate=0.06,\n",
    "    rnn_layers=1,\n",
    "    hidden_size=79,\n",
    "    dropout=0.46659,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "from pytorch_forecasting.metrics.point import MAE, SMAPE, RMSE\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_lstm = RecurrentNetwork.load_from_checkpoint(best_model_path)\n",
    "predictions = best_lstm.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "\n",
    "print(MAE()(predictions.output, predictions.y))\n",
    "print(SMAPE()(predictions.output, predictions.y))\n",
    "print(RMSE()(predictions.output, predictions.y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
